{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 5    \n",
    "## Собираем поисковик \n",
    "\n",
    "![](https://bilimfili.com/wp-content/uploads/2017/06/bir-urune-emek-vermek-o-urune-olan-deger-algimizi-degistirir-mi-bilimfilicom.jpg) \n",
    "\n",
    "\n",
    "Мы уже все знаем, для того чтобы сделать поисковик. Осталось соединить все части вместе.    \n",
    "Итак, для поисковика нам понадобятся:         \n",
    "**1. База документов **\n",
    "> в первом дз - корпус Друзей    \n",
    "в сегодняшнем дз - корпус юридических вопросов-ответов    \n",
    "в итоговом проекте - корпус Авито   \n",
    "\n",
    "**2. Функция индексации**                 \n",
    "Что делает: собирает информацию о корпусе, по которуму будет происходить поиск      \n",
    "Своя для каждого поискового метода:       \n",
    "> A. для обратного индекса она создает обратный индекс (чудо) и сохраняет статистики корпуса, необходимые для Okapi BM25 (средняя длина документа в коллекции, количество доков ... )             \n",
    "> B. для поиска через word2vec эта функция создает вектор для каждого документа в коллекции путем, например, усреднения всех векторов коллекции       \n",
    "> C. для поиска через doc2vec эта функция создает вектор для каждого документа               \n",
    "\n",
    "   Не забывайте сохранить все, что насчитает эта функция. Если это будет происходить налету во время поиска, понятно, что он будет работать сто лет     \n",
    "   \n",
    "**3. Функция поиска**     \n",
    "Можно разделить на две части:\n",
    "1. функция вычисления близости между запросом и документом    \n",
    "> 1. для индекса это Okapi BM25\n",
    "> 2. для w2v и d2v это обычная косинусная близость между векторами          \n",
    "2. ранжирование (или просто сортировка)\n",
    "\n",
    "\n",
    "Время все это реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МНОГИЕ ФУНКЦИИ ПОДПРАВЛЕНЫ В ДЗ4 (лучше туда смотреть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Индексация\n",
    "## Word2Vec\n",
    "### Задание 1\n",
    "Загрузите любую понравившуюся вам word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import judicial_splitter\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import operator\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если модель без тэгов\n",
    "model_without_pos = Word2Vec.load('/Users/irene/Downloads/IR/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если модель с POS-тэггингом\n",
    "# model_with_pos = KeyedVectors.load_word2vec_format('/Users/irene/Downloads/IR/tayga_1_2.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 \n",
    "Напишите функцию индексации для поиска через word2vec. Она должна для каждого документа из корпуса строить вектор.   \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только  вектор, но и опознователь текста, которому он принадлежит. \n",
    "Для поисковика это может быть url страницы, для поиска по текстовому корпусу сам текст.\n",
    "\n",
    "> В качестве документа для word2vec берите **параграфы** исходного текста, а не весь текст целиком. Так вектора будут более осмысленными. В противном случае можно получить один очень общий вектор, релевантый совершенно разным запросам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors_paragraph(paragraph, model):\n",
    "    \"\"\"Получает вектор для параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph)\n",
    "    if len(lemmas_paragraph) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        vector_paragraph = []\n",
    "        for lemma in lemmas_paragraph:\n",
    "            try:\n",
    "                vector = model.wv[lemma]\n",
    "            except:\n",
    "                vector = np.zeros(300)\n",
    "            vector_paragraph.append(vector)\n",
    "        vec = np.array(vector_paragraph).sum(axis=0) / len(vector_paragraph)\n",
    "        return vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors_text(text, model, len_par=4):\n",
    "    \"\"\"Получает массив векторов параграфов\"\"\"\n",
    "    paragraphs = judicial_splitter.splitter(text, len_par)\n",
    "    return [(paragraph, get_w2v_vectors_paragraph(paragraph, model)) for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_w2v_base(files_list, model):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    df = {'Name of file': [], 'Text of paragraph': [], 'Vector of paragraph (w2v)': []}\n",
    "    for file in tqdm(files_list):\n",
    "        with open(file, 'r') as f:  \n",
    "            text = f.read() \n",
    "            v_paragraphs = get_w2v_vectors_text(text, model)\n",
    "            for v_p in v_paragraphs:\n",
    "                df['Name of file'].append(file)\n",
    "                df['Text of paragraph'].append(v_p[0])\n",
    "                df['Vector of paragraph (w2v)'].append(v_p[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_dir = '/Users/irene/Downloads/IR/article'\n",
    "files_list = []\n",
    "for root, dirs, files in os.walk(main_dir):\n",
    "    for name in files:\n",
    "        if not '.DS_Store' in name:\n",
    "            files_list.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = save_w2v_base(files_list, model_without_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "### Задание 3\n",
    "Напишите функцию обучения doc2vec на юридических текстах, и получите свою кастомную d2v модель. \n",
    "> Совет: есть мнение, что для обучения doc2vec модели не нужно удалять стоп-слова из корпуса. Они являются важными семантическими элементами.      \n",
    "\n",
    "Важно! В качестве документа для doc2vec берите **параграфы** исходного текста, а не весь текст целиком. И не забывайте про предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(data):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=100, workers=4, dm=1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"model_doc2vec_5\")\n",
    "model_doc2vec = train_doc2vec(df['Text of paragraph'])\n",
    "model_doc2vec.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_doc2vec = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4\n",
    "Напишите функцию индексации для поиска через doc2vec. Она должна для каждого документа из корпуса получать вектор.    \n",
    "Все вектора надо сохранить, по формату советую json. При сохранении не забывайте, что вам надо сохранить не только вектор, но и опознователь текста, которому он принадлежит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(paragraph, model_doc2vec):\n",
    "    \"\"\"Получает вектор параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph, del_stopwords=False)\n",
    "    model_doc2vec.random.seed(100)\n",
    "    vector = model_doc2vec.infer_vector(lemmas_paragraph)\n",
    "    return vector.tolist()\n",
    "\n",
    "def save_d2v_base(paragraphs, model_doc2vec):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    vectors_d2v = []\n",
    "    for par in tqdm(paragraphs):\n",
    "        vectors_d2v.append(get_d2v_vectors(par, model_doc2vec))\n",
    "    return vectors_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Vector of paragraph (d2v)'] = save_d2v_base(df['Text of paragraph'], model_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name of file</th>\n",
       "      <th>Text of paragraph</th>\n",
       "      <th>Vector of paragraph (w2v)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/Users/irene/Downloads/IR/article/1.txt</td>\n",
       "      <td>Судья Высшего Арбитражного Суда Российской Ф...</td>\n",
       "      <td>[0.025288590806380136, 0.010929809320750314, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/irene/Downloads/IR/article/1.txt</td>\n",
       "      <td>Возвращение к проверке в порядке надзора судеб...</td>\n",
       "      <td>[0.01896677758187464, 0.011468302054260538, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/irene/Downloads/IR/article/10.txt</td>\n",
       "      <td>Судья Высшего Арбитражного Суда Российской Ф...</td>\n",
       "      <td>[0.02874657063457732, 0.01592562360706452, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/irene/Downloads/IR/article/100.txt</td>\n",
       "      <td>Высший Арбитражный Суд Российской Федерации ...</td>\n",
       "      <td>[0.03278444460856037, 0.012549695896825964, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/Users/irene/Downloads/IR/article/100.txt</td>\n",
       "      <td>Председательствующий судья Е.Н.ЗАРУБИНА Судья ...</td>\n",
       "      <td>[0.01786809911330541, 0.03585122898221016, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               Name of file  \\\n",
       "0           0    /Users/irene/Downloads/IR/article/1.txt   \n",
       "1           1    /Users/irene/Downloads/IR/article/1.txt   \n",
       "2           2   /Users/irene/Downloads/IR/article/10.txt   \n",
       "3           3  /Users/irene/Downloads/IR/article/100.txt   \n",
       "4           4  /Users/irene/Downloads/IR/article/100.txt   \n",
       "\n",
       "                                   Text of paragraph  \\\n",
       "0    Судья Высшего Арбитражного Суда Российской Ф...   \n",
       "1  Возвращение к проверке в порядке надзора судеб...   \n",
       "2    Судья Высшего Арбитражного Суда Российской Ф...   \n",
       "3    Высший Арбитражный Суд Российской Федерации ...   \n",
       "4  Председательствующий судья Е.Н.ЗАРУБИНА Судья ...   \n",
       "\n",
       "                           Vector of paragraph (w2v)  \n",
       "0  [0.025288590806380136, 0.010929809320750314, 0...  \n",
       "1  [0.01896677758187464, 0.011468302054260538, -0...  \n",
       "2  [0.02874657063457732, 0.01592562360706452, 0.0...  \n",
       "3  [0.03278444460856037, 0.012549695896825964, -0...  \n",
       "4  [0.01786809911330541, 0.03585122898221016, 0.0...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция поиска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обратного индекса функцией поиска является Okapi BM25. Она у вас уже должна быть реализована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция измерения близости между векторами нам пригодится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите функцию для поиска через word2vec и для поиска через doc2vec, которая по входящему запросу выдает отсортированную выдачу документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_v(vectors, names_doc, v_quary):\n",
    "    res = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        cos_sim = similarity(v_quary, vector)\n",
    "        res.append((names_doc[i], cos_sim))\n",
    "    res.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(quary, model, vectors_w2v, names_doc):\n",
    "    v_quary = get_w2v_vectors_paragraph(quary, model)\n",
    "    res = res_v(vectors_w2v, names_doc, v_quary)\n",
    "    return res\n",
    "\n",
    "def search_d2v(quary, model, vectors_d2v, names_doc):\n",
    "    v_quary = get_d2v_vectors(paragraph, model)\n",
    "    res = res_v(vectors_d2v, names_doc, v_quary)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#res = search_w2v('Судья Высшего Арбитражного Суда Российской Федерации Ю.Ю.Горячева, рассмотрев заявление Комитета по управлению городским имуществом Санкт-Петербурга', model_without_pos, df['Vector of paragraph (w2v)'][0:40000], df['Name of file'][0:40000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#res[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['Name of file'][35000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['Text of paragraph'][35000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = []\n",
    "for f in files_list:\n",
    "    with open(file, 'r') as f:  \n",
    "        text = f.read()\n",
    "        answers.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_texts = []\n",
    "for each_f in tqdm(answers):\n",
    "    lemmatized = ' '.join([x for x in preprocessing(each_f) if x != ' '])\n",
    "    lemmatized_texts.append(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(lemmatized_texts)\n",
    "df_index = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "words = list(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index(df) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for word in df:\n",
    "        sub = []\n",
    "        docs = np.where(df[word] > 0)[0]\n",
    "        for f in docs:\n",
    "            dl = len(lemmatized_texts[f].split())\n",
    "            fr = round(df[word][f]/dl, 4)\n",
    "            sub.append([f, dl, fr])\n",
    "        files.append(sub)\n",
    "    index = pd.DataFrame(data={'Слово': words, 'Информация': files})\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = inverted_index(df_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 2.0\n",
    "b = 0.75\n",
    "avgdl = round(sum([len(q.split(' ')) for q in lemmatized_texts])/len(lemmatized_texts))#средняя длина док-ов в коллекции\n",
    "N = len(lemmatized_texts)\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = math.log((N-n+0.5)/(n+0.5)) * (k1+1)*qf/(qf+k1*(1-b+b*(dl/avgdl)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(lemma, inverted_index) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between word in search query and all document  from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    #print(list(inverted_index.loc[inverted_index['Слово'] == lemma]['Информация']))\n",
    "    doc_list = list(inverted_index.loc[inverted_index['Слово'] == lemma]['Информация'])[0]\n",
    "    relevance_dict = {}\n",
    "    for doc in doc_list:\n",
    "        relevance_dict[doc[0]] = score_BM25(doc[2], doc[1], avgdl, k1, b, N, len(doc_list))\n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, top=5) -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    query = [que for que in preprocessing(query) if que in words]\n",
    "    #print(query)\n",
    "    res = {}\n",
    "    for word in query:\n",
    "        relevance_dict = compute_sim(word, index)\n",
    "        res = {k: res.get(k, 0) + relevance_dict.get(k, 0) for k in set(res) | set(relevance_dict)}\n",
    "    return sorted(res.items(), key=operator.itemgetter(1), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения всех этих заданий ваш поисковик готов, поздравляю!                  \n",
    "Осталось завернуть все написанное в питон скрипт, и сделать общую функцию поиска гибким, чтобы мы могли искать как по обратному индексу, так и по word2vec, так и по doc2vec.          \n",
    "Сделать это можно очень просто через старый добрый ``` if ```, который будет дергать ту или иную функцию поиска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(search_method):\n",
    "    if search_method == 'inverted_index':\n",
    "        search_result = get_search_result(query)\n",
    "    elif search_method == 'word2vec':\n",
    "        search_result = search_w2v()\n",
    "    elif search_method == 'doc2vec':\n",
    "        search_result = search_d2v()\n",
    "    else:\n",
    "        raise TypeError('unsupported search method')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
