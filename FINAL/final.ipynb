{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import judicial_splitter\n",
    "import collections\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "from math import log\n",
    "import pandas as pd\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import operator\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from model import Model\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "from operator import itemgetter\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_pos = Word2Vec.load('/Users/irene/Downloads/IR/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "fname = get_tmpfile(\"model_doc2vec_avito\")\n",
    "model_doc2vec = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('avito_df.csv')\n",
    "df = pd.read_csv('avito_df_vecors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_texts = []\n",
    "for x in data['lemmatized']:\n",
    "    if type(x) != str:\n",
    "        lemmatized_texts.append('')\n",
    "    else:\n",
    "        lemmatized_texts.append(x)\n",
    "corpus = lemmatized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_list(string):\n",
    "    string = string.strip('[]')\n",
    "    string = string.replace('\\n', '')\n",
    "    if ', ' not in string:\n",
    "        l = string.split(' ')\n",
    "    else:\n",
    "        l = string.split(', ')\n",
    "    return [float(e) for e in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w2v'] = df['w2v'].apply(to_list)\n",
    "df['w2v_tfidf'] = df['w2v_tfidf'].apply(to_list)\n",
    "df['d2v_hypo'] = df['d2v_hypo'].apply(to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    if del_digit:\n",
    "        input_text = re.sub('[0-9]', '', input_text)\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    tf_text = collections.Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_tfidf(word, quary, corpus):\n",
    "    try:\n",
    "        quary = preprocessing(quary)\n",
    "        computed_tf = compute_tf(quary)[word]\n",
    "        #print(computed_tf)\n",
    "        tfidf = computed_tf * compute_idf(word, corpus)\n",
    "    except:\n",
    "        tfidf = 0.0\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors_paragraph(paragraph, model, tfidf, ind, multiply_tfidf=True, pos=False):\n",
    "    \"\"\"Получает вектор для параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph)\n",
    "    #print('lemmas_paragraph', lemmas_paragraph)\n",
    "    if len(lemmas_paragraph) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        vector_paragraph = []\n",
    "        for lemma in lemmas_paragraph:\n",
    "            if pos:\n",
    "                lemma = lemma + '_' + get_pos(lemma)\n",
    "            try:\n",
    "                if multiply_tfidf:\n",
    "                    #print(lemma)\n",
    "                    tfidf = compute_new_tfidf(lemma, paragraph, corpus)\n",
    "                    #print(lemma, tfidf)\n",
    "                    vector = model.wv[lemma] * tfidf\n",
    "                else:\n",
    "                    vector = model.wv[lemma]\n",
    "            except:\n",
    "                vector = np.zeros(300)\n",
    "            vector_paragraph.append(vector)\n",
    "        vec = np.array(vector_paragraph).sum(axis=0) / len(vector_paragraph)\n",
    "        return vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(paragraph, model_doc2vec, steps=5, alpha=0.1):\n",
    "    \"\"\"Получает вектор параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph, del_stopwords=False)\n",
    "    model_doc2vec.random.seed(100)\n",
    "    vector = model_doc2vec.infer_vector(lemmas_paragraph, steps=steps, alpha=alpha)\n",
    "    return vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_v(vectors, names_doc, v_quary):\n",
    "    res = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        cos_sim = similarity(v_quary, vector)\n",
    "        res.append([names_doc[i], cos_sim, i])\n",
    "    res.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_without_dupl(res, top=5):\n",
    "    res_without_dupl = set()\n",
    "    inds = []\n",
    "    for ind, r in enumerate(res):\n",
    "        if r[0] in res_without_dupl:\n",
    "            continue\n",
    "        else:\n",
    "            if len(res_without_dupl) == top:\n",
    "                break\n",
    "            res_without_dupl.add(r[0])\n",
    "            inds.append(ind)\n",
    "        ind += 1\n",
    "    return itemgetter(*inds)(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(quary, model, vectors_w2v, names_doc, tfidf, ind, multiply_tfidf=True, pos=False, top=5):\n",
    "    v_quary = get_w2v_vectors_paragraph(quary, model, tfidf, ind, multiply_tfidf=multiply_tfidf, pos=pos)\n",
    "    #print(v_quary)\n",
    "    res = res_v(vectors_w2v, names_doc, v_quary)\n",
    "    res = res_without_dupl(res, top=top)\n",
    "    return res\n",
    "\n",
    "def search_d2v(quary, model, vectors_d2v, names_doc, steps=5, alpha=0.1, top=5):\n",
    "    v_quary = get_d2v_vectors(quary, model, steps=steps, alpha=alpha)\n",
    "    res = res_v(vectors_d2v, names_doc, v_quary)\n",
    "    res = res_without_dupl(res, top=top)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(lemmatized_texts)\n",
    "df_index = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "words = list(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index(df) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for word in df:\n",
    "        sub = []\n",
    "        docs = np.where(df[word] > 0)[0]\n",
    "        for f in docs:\n",
    "            dl = len(lemmatized_texts[f].split())\n",
    "            fr = round(df[word][f]/dl, 4)\n",
    "            sub.append([f, dl, fr])\n",
    "        files.append(sub)\n",
    "    index = pd.DataFrame(data={'Слово': words, 'Информация': files})\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = inverted_index(df_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "avgdl = round(sum([len(q.split(' ')) for q in lemmatized_texts])/len(lemmatized_texts))#средняя длина док-ов в коллекции\n",
    "N = len(lemmatized_texts)\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = math.log((N-n+0.5)/(n+0.5)) * (k1+1)*qf/(qf+k1*(1-b+b*(dl/avgdl)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(lemma, index) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between word in search query and all document  from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    doc_list = list(index.loc[index['Слово'] == lemma]['Информация'])[0]\n",
    "    #print(len(doc_list))\n",
    "    relevance_dict = {}\n",
    "    for doc in doc_list:\n",
    "        relevance_dict[doc[0]] = score_BM25(doc[2], doc[1], avgdl, k1, b, N, len(doc_list))\n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, top=5) -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    query = [que for que in preprocessing(query) if que in words]\n",
    "    #print(query)\n",
    "    res = {}\n",
    "    for word in query:\n",
    "        #print(word)\n",
    "        relevance_dict = compute_sim(word, index)\n",
    "        #print(relevance_dict)\n",
    "        res = {k: res.get(k, 0) + relevance_dict.get(k, 0) for k in set(res) | set(relevance_dict)}\n",
    "    return sorted(res.items(), key=operator.itemgetter(1), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_d2v_w2v(res_w2v, res_d2v, v, top=5):\n",
    "    res_w2v = sorted(res_w2v, key = lambda x: (x[0], x[2]))\n",
    "    res_d2v = sorted(res_d2v, key = lambda x: (x[0], x[2]))\n",
    "    ranges = []\n",
    "    for i, res3 in enumerate(res_w2v):\n",
    "        new_range = res3[1] * v + res_d2v[i][1] * (1-v)\n",
    "        ranges.append((res3[0], new_range))\n",
    "    return sorted(ranges, key = lambda x: (x[1]), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(vector):\n",
    "    a = np.asarray(vector)\n",
    "    return np.interp(a, (a.min(), a.max()), (-1, +1))\n",
    "def mean_w2v(res_w2v):\n",
    "    df = pd.DataFrame(list(res_w2v))\n",
    "    df = df.groupby(0)[1].agg([\"count\", \"sum\", \"mean\"])\n",
    "    m = df['mean']\n",
    "    return [(i, el) for i, el in enumerate(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_w2v_index(res_w2v, res_index, v, top=5):\n",
    "    res_w2v = mean_w2v(res_w2v)\n",
    "    #print(res_w2v)\n",
    "    res_w2v = sorted(res_w2v, key = lambda x: (x[0]))\n",
    "    res_index = sorted(res_index, key = lambda x: (x[0]))\n",
    "    files_ind = [r[0] for r in res_index]\n",
    "    res_w2v = np.asarray(res_w2v)[files_ind]\n",
    "    res_w2v_norm = [(i, j) for i, j in enumerate(norm([l[1] for l in res_w2v]))]\n",
    "    res_index_norm = [(i, j) for i, j in enumerate(norm([d[1] for d in res_index]))]\n",
    "    ranges = []\n",
    "    for i, res3 in enumerate(res_w2v_norm):\n",
    "        new_range = res3[1] * v + res_index_norm[i][1] * (1-v)\n",
    "        ranges.append((res3[0], new_range))\n",
    "    return sorted(ranges, key = lambda x: (x[1]), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(search_method, query, model_without_pos, model_doc2vec, vectors_w2v, vectors_d2v, names_doc, tfidf, ind_text, num_par_coll, num_text_coll, top=5):\n",
    "    try:\n",
    "        if search_method == 'inverted_index':\n",
    "            search_result = get_search_result(query, top=top)\n",
    "        elif search_method == 'word2vec':\n",
    "            search_result = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=top)\n",
    "        elif search_method == 'doc2vec':\n",
    "            search_result = search_d2v(query, model_doc2vec, vectors_d2v, names_doc, steps=5, alpha=0.1, top=top)\n",
    "        elif search_method == 'doc2vec+word2vec':\n",
    "            top_w2v = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=num_par_coll)\n",
    "            top_d2v = search_d2v(query, model_doc2vec, vectors_d2v, names_doc, steps=5, alpha=0.1, top=num_par_coll)\n",
    "            search_result = blend_d2v_w2v(top_w2v, top_d2v, v=0.5, top=5)\n",
    "        elif search_method == 'word2vec+inverted_index':\n",
    "            top_w2v = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=num_par_coll)\n",
    "            top_ind = get_search_result(query, top=num_text_coll)\n",
    "            search_result = blend_w2v_index(top_w2v, top_ind, v=0.5, top=5)\n",
    "        else:\n",
    "            raise TypeError('unsupported search method')\n",
    "    except:\n",
    "        search_result = 'Неправильный запрос!'\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_method = 'word2vec+inverted_index'\n",
    "query = 'Боится'\n",
    "answers = search(search_method, query, model_without_pos, model_doc2vec, df['w2v_tfidf'], df['d2v_hypo'], df['id_answer'], 'tfidf', 'del', len(df['id_answer']), len(data['description']), top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9201042627083805),\n",
       " (21, 0.5381342299648765),\n",
       " (7, 0.23127271133912897),\n",
       " (22, 0.1682749919218684),\n",
       " (27, 0.1239168743162885)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_ans(answers):\n",
    "    res = []\n",
    "    for ans in [g[0] for g in answers]:\n",
    "        title = data.iloc[ans]['title']\n",
    "        num_date = data.iloc[ans]['num_date']\n",
    "        author = data.iloc[ans]['author']\n",
    "        address = data.iloc[ans]['address']\n",
    "        breed = data.iloc[ans]['breed']\n",
    "        price = data.iloc[ans]['price']\n",
    "        description = data.iloc[ans]['description']\n",
    "        one = [title, num_date, author, address, breed, price, description]\n",
    "        res.append(one)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пекинес No 925865570, размещено вчера в 07:46\n",
      "Такса ищет подругу No 780980520, размещено вчера в 08:12\n",
      "Сибирская хаски No 1385306719, размещено 3 октября в 10:19\n",
      "Вязка, кобель No 1189295773, размещено вчера в 07:44\n",
      "Кавалер Кинг Чарльз спаниель No 1013513683, размещено 3 октября в 20:24\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
