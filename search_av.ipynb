{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ по поиску\n",
    "\n",
    "Привет! Вам надо реализивать поисковик на базе вопросов-ответов с сайта [pravoved.ru](https://pravoved.ru/questions-archive/).        \n",
    "Поиск должен работать на трех технологиях:       \n",
    "1. обратном индексе     \n",
    "2. word2vec         \n",
    "3. doc2vec      \n",
    "\n",
    "Вы должны понять, какой метод и при каких условиях эксперимента на этом корпусе работает лучше.          \n",
    "Для измерения качества поиска найдите точность (accuracy) выпадания правильного ответа на конкретный вопрос (в этой базе у каждого вопроса есть только один правильный ответ). Точность нужно измерить для всей базы.    \n",
    "При этом давайте считать, что выпал правильный ответ, если он попал в **топ-5** поисковой выдачи.\n",
    "\n",
    "> Сделайте ваш поиск максимально качественным, чтобы значение точности стремилось к 1.     \n",
    "Для этого можно поэкспериментировать со следующим:       \n",
    "- модель word2vec (можно брать любую из опен сорса или обучить свою)\n",
    "- способ получения вектора документа через word2vec: простое среднее арифметическое или взвешивать каждый вектор в соответствии с его tf-idf      \n",
    "- количество эпох у doc2vec (начинайте от 100)\n",
    "- предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)\n",
    "- блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v\n",
    "\n",
    "На это задание отведем 10 дней. Дэдлайн сдачи до полуночи 12.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import judicial_splitter\n",
    "import collections\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import operator\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from model import Model\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_dir = '/Users/irene/Downloads/avito_dogs'\n",
    "files_list = []\n",
    "for root, dirs, files in os.walk(main_dir):\n",
    "    for name in files:\n",
    "        if not '.DS_Store' in name:\n",
    "            files_list.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_parse(path):\n",
    "    with open(path, 'r') as f:  \n",
    "        info = f.readlines()\n",
    "    title = info[0].replace('Название: ', '').strip()\n",
    "    num_date = info[1].replace('Номер и дата объявления: ', '').strip()\n",
    "    author = info[2].replace('Информация об авторе: ', '').strip()\n",
    "    address = info[3].replace('Адрес собаки: ', '').strip().strip()\n",
    "    breed = info[4].replace('Порода: ', '').strip()\n",
    "    price = info[5].replace('Цена: ', '').strip()\n",
    "    description = info[6].replace('Описание собаки: ', '').strip()\n",
    "    url = info[7].replace('URL: ', '').strip()\n",
    "    return title, num_date, author, address, breed, price, description, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_parse(files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['title', 'num_date', 'author', 'address', 'breed', 'price', 'description', 'url'])\n",
    "for i, path in tqdm(enumerate(files_list)):\n",
    "    title, num_date, author, address, breed, price, description, url = read_parse(path)\n",
    "    df.loc[i] = [title, num_date, author, address, breed, price, description, url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('avito_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('avito_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>num_date</th>\n",
       "      <th>author</th>\n",
       "      <th>address</th>\n",
       "      <th>breed</th>\n",
       "      <th>price</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Гончая</td>\n",
       "      <td>No 1710785348, размещено 3 октября в 17:35</td>\n",
       "      <td>Юлия, Частное лицо, На Авито c сентября 2015, ...</td>\n",
       "      <td>Абакан,</td>\n",
       "      <td>лабрадор</td>\n",
       "      <td>100 ₽</td>\n",
       "      <td>Найден Калинино 3, пес очень умный, знает ко...</td>\n",
       "      <td>https://www.avito.ru/abakan/sobaki/gonchaya_17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Пекинес</td>\n",
       "      <td>No 925865570, размещено вчера в 07:46</td>\n",
       "      <td>Анна, Частное лицо, На Авито c марта 2014, Зав...</td>\n",
       "      <td>Абакан,</td>\n",
       "      <td>пекинес</td>\n",
       "      <td>10 000 ₽</td>\n",
       "      <td>Щенки Пекинеса 2 кобеля. Дата рождения 11 мая ...</td>\n",
       "      <td>https://www.avito.ru/abakan/sobaki/pekines_925...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Померанский Шпиц бело крем, белые</td>\n",
       "      <td>No 824574165, размещено 3 октября в 22:54</td>\n",
       "      <td>алексей, На Авито c января 2016, Завершено 52...</td>\n",
       "      <td>Абакан,</td>\n",
       "      <td>шпиц</td>\n",
       "      <td>26 000 ₽</td>\n",
       "      <td>Продаются щенки померанского шпица. бело кремо...</td>\n",
       "      <td>https://www.avito.ru/abakan/sobaki/pomeranskiy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Продажа собаки</td>\n",
       "      <td>No 565618762, размещено 3 октября в 20:48</td>\n",
       "      <td>Наталья, Частное лицо, На Авито c апреля 2013,...</td>\n",
       "      <td>Абакан,</td>\n",
       "      <td>другая</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>ждут хорошего хозяина, собака умная, чуткая, с...</td>\n",
       "      <td>https://www.avito.ru/abakan/sobaki/prodazha_so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Продажа собаки</td>\n",
       "      <td>No 596714096, размещено 3 октября в 20:45</td>\n",
       "      <td>Наталья, Частное лицо, На Авито c апреля 2013,...</td>\n",
       "      <td>Абакан,</td>\n",
       "      <td>другая</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>украшение для вашей семьи</td>\n",
       "      <td>https://www.avito.ru/abakan/sobaki/prodazha_so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               title  \\\n",
       "0           0                              Гончая   \n",
       "1           1                             Пекинес   \n",
       "2           2  Померанский Шпиц бело крем, белые   \n",
       "3           3                      Продажа собаки   \n",
       "4           4                      Продажа собаки   \n",
       "\n",
       "                                     num_date  \\\n",
       "0  No 1710785348, размещено 3 октября в 17:35   \n",
       "1       No 925865570, размещено вчера в 07:46   \n",
       "2   No 824574165, размещено 3 октября в 22:54   \n",
       "3   No 565618762, размещено 3 октября в 20:48   \n",
       "4   No 596714096, размещено 3 октября в 20:45   \n",
       "\n",
       "                                              author  address     breed  \\\n",
       "0  Юлия, Частное лицо, На Авито c сентября 2015, ...  Абакан,  лабрадор   \n",
       "1  Анна, Частное лицо, На Авито c марта 2014, Зав...  Абакан,   пекинес   \n",
       "2  алексей, На Авито c января 2016, Завершено 52...  Абакан,      шпиц   \n",
       "3  Наталья, Частное лицо, На Авито c апреля 2013,...  Абакан,    другая   \n",
       "4  Наталья, Частное лицо, На Авито c апреля 2013,...  Абакан,    другая   \n",
       "\n",
       "        price                                        description  \\\n",
       "0       100 ₽  Найден Калинино 3, пес очень умный, знает ко...   \n",
       "1    10 000 ₽  Щенки Пекинеса 2 кобеля. Дата рождения 11 мая ...   \n",
       "2    26 000 ₽  Продаются щенки померанского шпица. бело кремо...   \n",
       "3  Договорная  ждут хорошего хозяина, собака умная, чуткая, с...   \n",
       "4  Договорная                         украшение для вашей семьи   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.avito.ru/abakan/sobaki/gonchaya_17...  \n",
       "1  https://www.avito.ru/abakan/sobaki/pekines_925...  \n",
       "2  https://www.avito.ru/abakan/sobaki/pomeranskiy...  \n",
       "3  https://www.avito.ru/abakan/sobaki/prodazha_so...  \n",
       "4  https://www.avito.ru/abakan/sobaki/prodazha_so...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(input_text, del_stopwords=True, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "    if del_digit:\n",
    "        input_text = re.sub('[0-9]', '', input_text)\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [mystem.lemmatize(x)[0] for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    tf_text = collections.Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_idf(word, corpus):\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    documents_list = []\n",
    "    for text in tqdm(corpus):\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list\n",
    "\n",
    "#corpus = [['pasta', 'la', 'vista', 'baby', 'la', 'vista'], \n",
    "#['hasta', 'siempre', 'comandante', 'baby', 'la', 'siempre'], \n",
    "#['siempre', 'comandante', 'baby', 'la', 'siempre']]\n",
    "#compute_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae97cfd31ff04865862ac04ae1941bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# description\n",
    "# num_date, price, title, address, breed, author  - доп признаки (отдельно обрабатываются)\n",
    "corpus = [preprocessing(d) for d in data['description']]\n",
    "tfidf = compute_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_tfidf(word, quary, corpus):\n",
    "    try:\n",
    "        quary = preprocessing(quary)\n",
    "        computed_tf = compute_tf(quary)[word]\n",
    "        tfidf = computed_tf * compute_idf(word, corpus)\n",
    "    except:\n",
    "        tfidf = 0.0\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если модель без тэгов\n",
    "model_without_pos = Word2Vec.load('/Users/irene/Downloads/IR/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors_paragraph(paragraph, model, tfidf, ind, multiply_tfidf=True, pos=False):\n",
    "    \"\"\"Получает вектор для параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph)\n",
    "    #print('lemmas_paragraph', lemmas_paragraph)\n",
    "    if len(lemmas_paragraph) == 0:\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        vector_paragraph = []\n",
    "        for lemma in lemmas_paragraph:\n",
    "            if pos:\n",
    "                lemma = lemma + '_' + get_pos(lemma)\n",
    "            try:\n",
    "                if multiply_tfidf:\n",
    "                    tfidf = compute_new_tfidf(lemma, paragraph, corpus)\n",
    "                    #print(lemma, tfidf)\n",
    "                    vector = model.wv[lemma] * tfidf\n",
    "                else:\n",
    "                    vector = model.wv[lemma]\n",
    "            except:\n",
    "                vector = np.zeros(300)\n",
    "            vector_paragraph.append(vector)\n",
    "        vec = np.array(vector_paragraph).sum(axis=0) / len(vector_paragraph)\n",
    "        return vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_w2v_vectors_paragraph('коричневый бульдог', model_without_pos, 'dfn', 'djvsd', multiply_tfidf=True, pos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_vectors_text(text, model, tfidf, ind, len_par=4, multiply_tfidf=True, pos=False):\n",
    "    \"\"\"Получает массив векторов параграфов\"\"\"\n",
    "    paragraphs = judicial_splitter.splitter(text, len_par)\n",
    "    return [(paragraph, get_w2v_vectors_paragraph(paragraph, model, tfidf, ind, multiply_tfidf=multiply_tfidf, pos=pos)) for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_w2v_base(answers, model, tfidf, len_par=4, multiply_tfidf=True, pos=False):\n",
    "    \"\"\"Индексирует всю базу для поиска через word2vec\"\"\"\n",
    "    id_answer = []\n",
    "    text_of_paragraph = []\n",
    "    w2v = []\n",
    "    for i, answer in tqdm(enumerate(answers)):\n",
    "        v_paragraphs = get_w2v_vectors_text(answer, model, tfidf, i, len_par=len_par, multiply_tfidf=multiply_tfidf, pos=pos)\n",
    "        for v_p in v_paragraphs:\n",
    "            id_answer.append(i)\n",
    "            text_of_paragraph.append(v_p[0])\n",
    "            w2v.append(v_p[1])\n",
    "    return id_answer, text_of_paragraph, w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df(name_cols, data_cols):\n",
    "    df = {}\n",
    "    for i, name in enumerate(name_cols):\n",
    "        df[name] = data_cols[i]\n",
    "    df = pd.DataFrame(data=df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f04ed0a18e4a74b407e390afe0d2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_answer, text_of_paragraph, w2v = save_w2v_base(data['description'], model_without_pos, tfidf, len_par=2, multiply_tfidf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = create_df(['id_answer', 'text_of_paragraph', 'w2v'], [id_answer, text_of_paragraph, w2v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7db9e461034be59b9226eeaa121fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_answer_tfidf, text_of_paragraph_tfidf, w2v_tfidf = save_w2v_base(data['description'], model_without_pos, tfidf, len_par=2, multiply_tfidf=True)\n",
    "df['w2v_tfidf'] = w2v_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec(data, epochs=100):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, \n",
    "                min_alpha=0.025, epochs=epochs, workers=4, dm=1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"model_doc2vec_avito\")\n",
    "model_doc2vec = train_doc2vec(df['text_of_paragraph'], epochs=1000)\n",
    "model_doc2vec.save(fname)\n",
    "model_doc2vec = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d2v_vectors(paragraph, model_doc2vec, steps=5, alpha=0.1):\n",
    "    \"\"\"Получает вектор параграфа\"\"\"\n",
    "    lemmas_paragraph = preprocessing(paragraph, del_stopwords=False)\n",
    "    model_doc2vec.random.seed(100)\n",
    "    vector = model_doc2vec.infer_vector(lemmas_paragraph, steps=steps, alpha=alpha)\n",
    "    return vector.tolist()\n",
    "\n",
    "def save_d2v_base(paragraphs, model_doc2vec, steps=5, alpha=0.1):\n",
    "    \"\"\"Индексирует всю базу для поиска через doc2vec\"\"\"\n",
    "    vectors_d2v = []\n",
    "    for par in tqdm(paragraphs):\n",
    "        vectors_d2v.append(get_d2v_vectors(par, model_doc2vec, steps=steps, alpha=alpha))\n",
    "    return vectors_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a893c77b5e4ad09ad04ab74cd02e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['d2v_hypo'] = save_d2v_base(df['text_of_paragraph'], model_doc2vec, steps=10, alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_answer</th>\n",
       "      <th>text_of_paragraph</th>\n",
       "      <th>w2v</th>\n",
       "      <th>w2v_tfidf</th>\n",
       "      <th>d2v_hypo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Найден Калинино 3, пес очень умный, знает ко...</td>\n",
       "      <td>[0.0011089619947597384, -0.011527637019753456,...</td>\n",
       "      <td>[-0.0002580559521447867, -0.000550129043404012...</td>\n",
       "      <td>[0.5840355753898621, -0.32408908009529114, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Щенки Пекинеса 2 кобеля. Дата рождения 11 мая ...</td>\n",
       "      <td>[0.03224942533092366, 0.03836751843078269, 0.0...</td>\n",
       "      <td>[0.0009409348325182995, 0.0009336340889502834,...</td>\n",
       "      <td>[1.439154028892517, 0.5374758839607239, 0.6894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Воспитанные.росли в частном доме. На улицу про...</td>\n",
       "      <td>[-0.01494167186319828, 0.017454788088798523, 0...</td>\n",
       "      <td>[-0.0006672082236036658, 0.0011105554876849055...</td>\n",
       "      <td>[0.4032999873161316, -0.22038237750530243, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Питание смешанное. Очень привязанные к друг др...</td>\n",
       "      <td>[-0.007580178324133158, 0.001755695790052414, ...</td>\n",
       "      <td>[-0.0001709196512820199, 8.916016668081284e-06...</td>\n",
       "      <td>[0.07304802536964417, 0.14767879247665405, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Поэтому желательно в одну семью. Прежде чем вз...</td>\n",
       "      <td>[0.009978953748941422, 0.0028232107870280743, ...</td>\n",
       "      <td>[0.0005157773848623037, 0.0003232476010452956,...</td>\n",
       "      <td>[0.8697100281715393, 0.20530551671981812, 0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_answer                                  text_of_paragraph  \\\n",
       "0          0  Найден Калинино 3, пес очень умный, знает ко...   \n",
       "1          1  Щенки Пекинеса 2 кобеля. Дата рождения 11 мая ...   \n",
       "2          1  Воспитанные.росли в частном доме. На улицу про...   \n",
       "3          1  Питание смешанное. Очень привязанные к друг др...   \n",
       "4          1  Поэтому желательно в одну семью. Прежде чем вз...   \n",
       "\n",
       "                                                 w2v  \\\n",
       "0  [0.0011089619947597384, -0.011527637019753456,...   \n",
       "1  [0.03224942533092366, 0.03836751843078269, 0.0...   \n",
       "2  [-0.01494167186319828, 0.017454788088798523, 0...   \n",
       "3  [-0.007580178324133158, 0.001755695790052414, ...   \n",
       "4  [0.009978953748941422, 0.0028232107870280743, ...   \n",
       "\n",
       "                                           w2v_tfidf  \\\n",
       "0  [-0.0002580559521447867, -0.000550129043404012...   \n",
       "1  [0.0009409348325182995, 0.0009336340889502834,...   \n",
       "2  [-0.0006672082236036658, 0.0011105554876849055...   \n",
       "3  [-0.0001709196512820199, 8.916016668081284e-06...   \n",
       "4  [0.0005157773848623037, 0.0003232476010452956,...   \n",
       "\n",
       "                                            d2v_hypo  \n",
       "0  [0.5840355753898621, -0.32408908009529114, -0....  \n",
       "1  [1.439154028892517, 0.5374758839607239, 0.6894...  \n",
       "2  [0.4032999873161316, -0.22038237750530243, 0.0...  \n",
       "3  [0.07304802536964417, 0.14767879247665405, 0.3...  \n",
       "4  [0.8697100281715393, 0.20530551671981812, 0.06...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('avito_df_vecors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v + d2v (общие функции для поиска)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_v(vectors, names_doc, v_quary):\n",
    "    res = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        cos_sim = similarity(v_quary, vector)\n",
    "        res.append([names_doc[i], cos_sim, i])\n",
    "    res.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_without_dupl(res, top=5):\n",
    "    res_without_dupl = set()\n",
    "    inds = []\n",
    "    for ind, r in enumerate(res):\n",
    "        if r[0] in res_without_dupl:\n",
    "            continue\n",
    "        else:\n",
    "            if len(res_without_dupl) == top:\n",
    "                break\n",
    "            res_without_dupl.add(r[0])\n",
    "            inds.append(ind)\n",
    "        ind += 1\n",
    "    return itemgetter(*inds)(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = [[1, 0.9, 2], [1, 0.8, 1], [2, 0.7, 1], [2, 0.5, 3], [2, 0.5, 2], [3, 0.3, 1], [3, 0.2, 2], [4, 0.2, 1], [5, 0.2, 1], [6, 0.1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0.9, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_without_dupl(res, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_w2v(quary, model, vectors_w2v, names_doc, tfidf, ind, multiply_tfidf=True, pos=False, top=5):\n",
    "    v_quary = get_w2v_vectors_paragraph(quary, model, tfidf, ind, multiply_tfidf=multiply_tfidf, pos=pos)\n",
    "    #print(v_quary)\n",
    "    res = res_v(vectors_w2v, names_doc, v_quary)\n",
    "    res = res_without_dupl(res, top=top)\n",
    "    return res\n",
    "\n",
    "def search_d2v(quary, model, vectors_d2v, names_doc, steps=5, alpha=0.1, top=5):\n",
    "    v_quary = get_d2v_vectors(quary, model, steps=steps, alpha=alpha)\n",
    "    res = res_v(vectors_d2v, names_doc, v_quary)\n",
    "    res = res_without_dupl(res, top=top)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4365, 0.8449181136334749, 8815],\n",
       " [2528, 0.8337310792077939, 4962],\n",
       " [2128, 0.8176431519677047, 4039],\n",
       " [2345, 0.8081612212255139, 4555],\n",
       " [4675, 0.782490373118999, 9356])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_w2v(query, model_without_pos, df['w2v_tfidf'], df['id_answer'], 'sdfa', 'dfs', multiply_tfidf=True, pos=False, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Обратный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadd074911c443f6abb853904e587c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatized_texts = []\n",
    "for each_f in tqdm(data['description']):\n",
    "    lemmatized = ' '.join([x for x in preprocessing(each_f) if x != ' '])\n",
    "    lemmatized_texts.append(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(lemmatized_texts)\n",
    "df_index = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "words = list(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>abeliya</th>\n",
       "      <th>abigail</th>\n",
       "      <th>abkc</th>\n",
       "      <th>above</th>\n",
       "      <th>absoluteli</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>acti</th>\n",
       "      <th>adamant</th>\n",
       "      <th>adba</th>\n",
       "      <th>...</th>\n",
       "      <th>ярильд</th>\n",
       "      <th>яркий</th>\n",
       "      <th>яркия</th>\n",
       "      <th>ярко</th>\n",
       "      <th>ярославль</th>\n",
       "      <th>ярун</th>\n",
       "      <th>ярцево</th>\n",
       "      <th>ярый</th>\n",
       "      <th>ясно</th>\n",
       "      <th>яша</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  abeliya  abigail  abkc  above  absoluteli  absolutely  acti  adamant  \\\n",
       "0   0        0        0     0      0           0           0     0        0   \n",
       "1   0        0        0     0      0           0           0     0        0   \n",
       "2   0        0        0     0      0           0           0     0        0   \n",
       "3   0        0        0     0      0           0           0     0        0   \n",
       "4   0        0        0     0      0           0           0     0        0   \n",
       "\n",
       "   adba ...   ярильд  яркий  яркия  ярко  ярославль  ярун  ярцево  ярый  ясно  \\\n",
       "0     0 ...        0      0      0     0          0     0       0     0     0   \n",
       "1     0 ...        0      0      0     0          0     0       0     0     0   \n",
       "2     0 ...        0      0      0     0          0     0       0     0     0   \n",
       "3     0 ...        0      0      0     0          0     0       0     0     0   \n",
       "4     0 ...        0      0      0     0          0     0       0     0     0   \n",
       "\n",
       "   яша  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 12365 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5691, 12365)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index(df) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for word in df:\n",
    "        sub = []\n",
    "        docs = np.where(df[word] > 0)[0]\n",
    "        for f in docs:\n",
    "            dl = len(lemmatized_texts[f].split())\n",
    "            fr = round(df[word][f]/dl, 4)\n",
    "            sub.append([f, dl, fr])\n",
    "        files.append(sub)\n",
    "    index = pd.DataFrame(data={'Слово': words, 'Информация': files})\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = inverted_index(df_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Информация</th>\n",
       "      <th>Слово</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1298, 88, 0.0114], [3445, 45, 0.0222]]</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[1940, 103, 0.0097]]</td>\n",
       "      <td>abeliya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[158, 110, 0.0182]]</td>\n",
       "      <td>abigail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[2073, 64, 0.0156]]</td>\n",
       "      <td>abkc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[4791, 36, 0.0278]]</td>\n",
       "      <td>above</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Информация    Слово\n",
       "0  [[1298, 88, 0.0114], [3445, 45, 0.0222]]       aa\n",
       "1                     [[1940, 103, 0.0097]]  abeliya\n",
       "2                      [[158, 110, 0.0182]]  abigail\n",
       "3                      [[2073, 64, 0.0156]]     abkc\n",
       "4                      [[4791, 36, 0.0278]]    above"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 2.0\n",
    "b = 0.75\n",
    "avgdl = round(sum([len(q.split(' ')) for q in lemmatized_texts])/len(lemmatized_texts))#средняя длина док-ов в коллекции\n",
    "N = len(lemmatized_texts)\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = math.log((N-n+0.5)/(n+0.5)) * (k1+1)*qf/(qf+k1*(1-b+b*(dl/avgdl)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sim(lemma, inverted_index) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between word in search query and all document  from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    doc_list = list(inverted_index.loc[inverted_index['Слово'] == lemma]['Информация'])[0]\n",
    "    #print(len(doc_list))\n",
    "    relevance_dict = {}\n",
    "    for doc in doc_list:\n",
    "        relevance_dict[doc[0]] = score_BM25(doc[2], doc[1], avgdl, k1, b, N, len(doc_list))\n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_result(query, top=5) -> list:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    query = [que for que in preprocessing(query) if que in words]\n",
    "    #print(query)\n",
    "    res = {}\n",
    "    for word in query:\n",
    "        relevance_dict = compute_sim(word, index)\n",
    "        res = {k: res.get(k, 0) + relevance_dict.get(k, 0) for k in set(res) | set(relevance_dict)}\n",
    "    return sorted(res.items(), key=operator.itemgetter(1), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_search_result('бульдог', top=1384))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * w2v + d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_d2v_w2v(res_w2v, res_d2v, v, top=5):\n",
    "    res_w2v = sorted(res_w2v, key = lambda x: (x[0], x[2]))\n",
    "    res_d2v = sorted(res_d2v, key = lambda x: (x[0], x[2]))\n",
    "    ranges = []\n",
    "    for i, res3 in enumerate(res_w2v):\n",
    "        new_range = res3[1] * v + res_d2v[i][1] * (1-v)\n",
    "        ranges.append((res3[0], new_range))\n",
    "    return sorted(ranges, key = lambda x: (x[1]), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_w2v = [[2, 0.3 , 1], [2, 0.2, 2], [2, 0.1, 3], [1, 0.1, 1], [1, 0.1, 2]]\n",
    "res_d2v = [[2, 0.8, 2], [2, 0.7, 1], [1, 0.5, 1], [2, 0.2, 3], [1, 0.1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.37999999999999995),\n",
       " (2, 0.32),\n",
       " (1, 0.18),\n",
       " (2, 0.12000000000000001),\n",
       " (1, 0.1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_d2v_w2v(res_w2v, res_d2v, 0.8, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * w2v + inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(vector):\n",
    "    a = np.asarray(vector)\n",
    "    return np.interp(a, (a.min(), a.max()), (-1, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    m = x[1] / x[0]\n",
    "    print(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_w2v(res_w2v):\n",
    "    df = pd.DataFrame(list(res_w2v))\n",
    "    df = df.groupby(0)[1].agg([\"count\", \"sum\", \"mean\"])\n",
    "    m = df['mean']\n",
    "    return [(i, el) for i, el in enumerate(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = [[87, 0.7786801283473814, 168], [5004, 0.7450444104011303, 9957], [3057, 0.7438936781176754, 6184], [2333, 0.7431848368026404, 4536], [1921, 0.7246510872323191, 3610], [949, 0.7221993874777424, 1807]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7786801283473814),\n",
       " (1, 0.7221993874777424),\n",
       " (2, 0.7246510872323191),\n",
       " (3, 0.7431848368026404),\n",
       " (4, 0.7438936781176754),\n",
       " (5, 0.7450444104011303)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_w2v(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blend_w2v_index(res_w2v, res_index, v, top=5):\n",
    "    res_w2v = mean_w2v(res_w2v)\n",
    "    #print(res_w2v)\n",
    "    res_w2v = sorted(res_w2v, key = lambda x: (x[0]))\n",
    "    res_index = sorted(res_index, key = lambda x: (x[0]))\n",
    "    files_ind = [r[0] for r in res_index]\n",
    "    res_w2v = np.asarray(res_w2v)[files_ind]\n",
    "    res_w2v_norm = [(i, j) for i, j in enumerate(norm([l[1] for l in res_w2v]))]\n",
    "    res_index_norm = [(i, j) for i, j in enumerate(norm([d[1] for d in res_index]))]\n",
    "    ranges = []\n",
    "    for i, res3 in enumerate(res_w2v_norm):\n",
    "        new_range = res3[1] * v + res_index_norm[i][1] * (1-v)\n",
    "        ranges.append((res3[0], new_range))\n",
    "    return sorted(ranges, key = lambda x: (x[1]), reverse=True)[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_w2v = [[0, 0.5, 3], [0, 0.5, 2], [1, 0.3, 1], [1, 0.2, 2], [2, 0.2, 1], [3, 0.2, 1], [4, 0.1, 1], [0, 0.1, 1], [54, 0.1, 1]]\n",
    "res_index = [(1, 65), (0, 6), (2, 5), (4, 2), (3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.5624999999999999),\n",
       " (0, 0.06349206349206349),\n",
       " (2, -0.5773809523809524),\n",
       " (3, -0.6091269841269842),\n",
       " (4, -1.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_w2v_index(res_w2v, res_index, 0.5, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(search_method, query, model_without_pos, model_doc2vec, vectors_w2v, vectors_d2v, names_doc, tfidf, ind_text, num_par_coll, num_text_coll, top=5):\n",
    "    try:\n",
    "        if search_method == 'inverted_index':\n",
    "            search_result = get_search_result(query, top=top)\n",
    "        elif search_method == 'word2vec':\n",
    "            search_result = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=top)\n",
    "        elif search_method == 'doc2vec':\n",
    "            search_result = search_d2v(query, model_doc2vec, vectors_d2v, names_doc, steps=5, alpha=0.1, top=top)\n",
    "        elif search_method == 'doc2vec+word2vec':\n",
    "            top_w2v = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=num_par_coll)\n",
    "            top_d2v = search_d2v(query, model_doc2vec, vectors_d2v, names_doc, steps=5, alpha=0.1, top=num_par_coll)\n",
    "            search_result = blend_d2v_w2v(top_w2v, top_d2v, v=0.5, top=5)\n",
    "        elif search_method == 'word2vec+inverted_index':\n",
    "            top_w2v = search_w2v(query, model_without_pos, vectors_w2v, names_doc, tfidf, ind_text, multiply_tfidf=True, pos=False, top=num_par_coll)\n",
    "            top_ind = get_search_result(query, top=num_text_coll)\n",
    "            search_result = blend_w2v_index(top_w2v, top_ind, v=0.5, top=5)\n",
    "        else:\n",
    "            raise TypeError('unsupported search method')\n",
    "    except:\n",
    "        search_result = 'Неправильный запрос!'\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_method = 'word2vec+inverted_index'\n",
    "query = 'Боится'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = search(search_method, query, model_without_pos, model_doc2vec, df['w2v_tfidf'], df['d2v_hypo'], df['id_answer'], tfidf, 'del', len(df['id_answer']), len(data['description']), top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9201042619696476),\n",
       " (21, 0.5381342302685125),\n",
       " (7, 0.23127271141879036),\n",
       " (22, 0.16827498897532261),\n",
       " (27, 0.12391687330100565)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Щенки Пекинеса 2 кобеля. Дата рождения 11 мая 2018г.есть паспорт.привиты по возрасту. Воспитанные.росли в частном доме. На улицу просятся. Питание смешанное. Очень привязанные к друг другу. Поэтому желательно в одну семью. Прежде чем взять щенков, узнайте все о породе, осознаете что это члены семьи а не игрушка. Проверьте себя и всех ваших родственников в семье на аллергию!!! Щенки с отличными породными данными. Родители с родословной. Папа белый королевский Пекинес мама рыжая. (последние 2 фото) родители живут в одной семье со щенками'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['description'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
